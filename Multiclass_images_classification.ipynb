{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8YqN5zZxbnVQkyZWCZ8cS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/washaq00/TeethOrBraces/blob/master/Multiclass_images_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2fQOZ2UX2hNE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_dataset import get_dataloaders_cifar10, UnNormalize\n",
        "\n",
        "RANDOM_SEED = 123\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 50\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "mkJo-3Ra3276"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((120, 120)),\n",
        "    torchvision.transforms.RandomCrop((110, 110)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((120, 120)),\n",
        "    torchvision.transforms.CenterCrop((110, 110)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_fraction=0.1,\n",
        "    train_transforms=train_transforms,\n",
        "    test_transforms=test_transforms,\n",
        "    num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvyRsWlc4kOT",
        "outputId": "f91f8843-cdd8-4b9c-d847-43de73b1623c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "VGG_types = {\n",
        "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG16\": [\n",
        "        64,\n",
        "        64,\n",
        "        \"M\",\n",
        "        128,\n",
        "        128,\n",
        "        \"M\",\n",
        "        256,\n",
        "        256,\n",
        "        256,\n",
        "        \"M\",\n",
        "        512,\n",
        "        512,\n",
        "        512,\n",
        "        \"M\",\n",
        "        512,\n",
        "        512,\n",
        "        512,\n",
        "        \"M\",\n",
        "    ],\n",
        "    \"VGG19\": [\n",
        "        64,\n",
        "        64,\n",
        "        \"M\",\n",
        "        128,\n",
        "        128,\n",
        "        \"M\",\n",
        "        256,\n",
        "        256,\n",
        "        256,\n",
        "        256,\n",
        "        \"M\",\n",
        "        512,\n",
        "        512,\n",
        "        512,\n",
        "        512,\n",
        "        \"M\",\n",
        "        512,\n",
        "        512,\n",
        "        512,\n",
        "        512,\n",
        "        \"M\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(VGG, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv_layers = self.create_conv_layers(VGG_types[\"VGG16\"])\n",
        "\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(512*2*2, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=in_channels,\n",
        "                        out_channels=out_channels,\n",
        "                        kernel_size=(3, 3),\n",
        "                        stride=(1, 1),\n",
        "                        padding=(1, 1),\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(x),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "unAw4mYH5dbT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_1 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=3,\n",
        "                                out_channels=64,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=64,\n",
        "                                out_channels=64,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.block_2 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=64,\n",
        "                                out_channels=128,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=128,\n",
        "                                out_channels=128,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.block_3 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=128,\n",
        "                                out_channels=256,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=256,\n",
        "                                out_channels=256,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=256,\n",
        "                                out_channels=256,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "\n",
        "\n",
        "        self.block_4 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=256,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "\n",
        "        self.block_5 = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Conv2d(in_channels=512,\n",
        "                                out_channels=512,\n",
        "                                kernel_size=(3, 3),\n",
        "                                stride=(1, 1),\n",
        "                                padding=1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                   stride=(2, 2))\n",
        "        )\n",
        "\n",
        "        height, width = 3, 3 ## you may want to change that depending on the input image size\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512*height*width, 4096),\n",
        "            torch.nn.ReLU(True),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(4096, 4096),\n",
        "            torch.nn.ReLU(True),\n",
        "            torch.nn.Dropout(p=0.5),\n",
        "            torch.nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.torch.nn.Conv2d) or isinstance(m, torch.torch.nn.Linear):\n",
        "                torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.detach().zero_()\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((height, width))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        x = self.block_4(x)\n",
        "        x = self.block_5(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "\n",
        "        logits = self.classifier(x)\n",
        "        #probas = F.softmax(logits, dim=1)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FLTcbvZm8gVl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from time import perf_counter\n",
        "from tqdm.auto import tqdm #progress bar\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "def model_eval(model: torch.nn.Module,\n",
        "               test_data: torch.utils.data.DataLoader,\n",
        "               acc1):\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    loss, accuracy = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for x, y in tqdm(test_data):\n",
        "            test_logits = model(x)\n",
        "            loss +=  F.cross_entropy(test_logits, y)\n",
        "            accuracy += acc1(y_true=y,y_pred= test_logits.argmax(dim=1))\n",
        "\n",
        "        loss /= len(test_data)\n",
        "        accuracy /= len(test_data)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__,\n",
        "            \"model_loss\": loss.item(), #returns the value of tensor\n",
        "            \"model_acc\": accuracy}\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               train_data: torch.utils.data.DataLoader,\n",
        "               optimize,\n",
        "               acc):\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    tloss:float = 0\n",
        "    tacc:float = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch, (x, y) in enumerate(train_data):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_logits = model(x)\n",
        "        loss = F.cross_entropy(y_logits, y)\n",
        "        optimize.zero_grad()\n",
        "        loss.backward()\n",
        "        optimize.step()\n",
        "        model.eval()\n",
        "\n",
        "        tloss += loss.item()\n",
        "        tacc += acc(y_true=y, y_pred=y_logits.argmax(dim=1))\n",
        "    tloss /= len(train_data)\n",
        "    tacc /= len(train_data)\n",
        "\n",
        "    return tloss, tacc\n",
        "\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              test_data: torch.utils.data.DataLoader,\n",
        "              acc):\n",
        "    tloss: float = 0\n",
        "    tacc: float = 0\n",
        "    model.eval()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    for x, y in test_data:\n",
        "        with torch.inference_mode():\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            test_logits = model(x)\n",
        "            loss = F.cross_entropy(test_logits, y)\n",
        "\n",
        "            tloss += loss.item()\n",
        "            tacc += acc(y_true=y,y_pred=test_logits.argmax(dim=1))\n",
        "    tloss /= len(test_data)\n",
        "    tacc /= len(test_data)\n",
        "\n",
        "    return tloss, tacc\n",
        "\n",
        "\n",
        "def training_loop(model: torch.nn.Module,\n",
        "                  train: torch.utils.data.DataLoader,\n",
        "                  test: torch.utils.data.DataLoader,\n",
        "                  optimizer,\n",
        "                  acc,\n",
        "                  epochs: int = 5):\n",
        "\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []}\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           train_data = train,\n",
        "                                           optimize= optimizer,\n",
        "                                           acc = acc)\n",
        "\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "                                        test_data=test,\n",
        "                                        acc=acc)\n",
        "\n",
        "        print(f\"Epoch: {epoch} train loss: {train_loss}, train acc: {train_acc}, test loss: {test_loss}, test acc: {test_acc}\")\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "NC0153iH5-uT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "2jW5Xz6U6Cmf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downsample=None, stride=1):\n",
        "        super().__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(in_channels,\n",
        "                               out_channels,\n",
        "                               kernel_size=1,\n",
        "                               stride=1,\n",
        "                               padding=0,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels,\n",
        "                               out_channels,\n",
        "                               kernel_size=3,\n",
        "                               stride=stride,\n",
        "                               padding=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels,\n",
        "                               out_channels*self.expansion,\n",
        "                               kernel_size=1,\n",
        "                               stride=1,\n",
        "                               padding=0,\n",
        "                               bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import math\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3,bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        #ResNet Layers\n",
        "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
        "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*4, num_classes) #fully conected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
        "        downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels,\n",
        "                          out_channels * 4,\n",
        "                          kernel_size=1,\n",
        "                          stride=stride,\n",
        "                          bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4))\n",
        "\n",
        "        layers.append(block(self.in_channels, out_channels, downsample, stride))\n",
        "        self.in_channels = out_channels*4\n",
        "\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def ResNet50(pretrained=False, img_channels=3, num_classes=10):\n",
        "\n",
        "    model = ResNet(ResNetBlock, [3,4,6,3], img_channels, num_classes)\n",
        "\n",
        "    if pretrained:\n",
        "        pretrained_dict = model_zoo.load_url(model_urls['resnet50'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_dict.items()}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(pretrained_dict)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ONxGg2vrGzbh"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from distutils.version import LooseVersion as Version\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def set_deterministic(use_tensorcores=False):\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    if torch.__version__ <= Version(\"1.7\"):\n",
        "        torch.set_deterministic(True)\n",
        "    else:\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "        # The following are set to True by default and allow cards\n",
        "        # like the Ampere and newer to utilize tensorcores for\n",
        "        # convolutions and matrix multiplications, which can result\n",
        "        # in a significant speed-up. However, results may differ compared\n",
        "        # to card how don't use mixed precision via tensor cores.\n",
        "        torch.backends.cuda.matmul.allow_tf32 = use_tensorcores\n",
        "        torch.backends.cudnn.allow_tf32 = use_tensorcores\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.float().to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "ZAZNSsPqE9x3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, num_epochs, train_loader,\n",
        "                valid_loader, test_loader, optimizer,\n",
        "                device, logging_interval=50,\n",
        "                scheduler=None,\n",
        "                scheduler_on='valid_acc'):\n",
        "\n",
        "    start_time = time.time()\n",
        "    minibatch_loss_list, train_acc_list, valid_acc_list = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # ## FORWARD AND BACK PROP\n",
        "            logits = model(features)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # ## UPDATE MODEL PARAMETERS\n",
        "            optimizer.step()\n",
        "\n",
        "            # ## LOGGING\n",
        "            minibatch_loss_list.append(loss.item())\n",
        "            if not batch_idx % logging_interval:\n",
        "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
        "                      f'| Loss: {loss:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # save memory during inference\n",
        "            train_acc = compute_accuracy(model, train_loader, device=device)\n",
        "            valid_acc = compute_accuracy(model, valid_loader, device=device)\n",
        "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                  f'| Train: {train_acc :.2f}% '\n",
        "                  f'| Validation: {valid_acc :.2f}%')\n",
        "            train_acc_list.append(train_acc.item())\n",
        "            valid_acc_list.append(valid_acc.item())\n",
        "\n",
        "        elapsed = (time.time() - start_time)/60\n",
        "        print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "        if scheduler is not None:\n",
        "\n",
        "            if scheduler_on == 'valid_acc':\n",
        "                scheduler.step(valid_acc_list[-1])\n",
        "            elif scheduler_on == 'minibatch_loss':\n",
        "                scheduler.step(minibatch_loss_list[-1])\n",
        "            else:\n",
        "                raise ValueError(f'Invalid `scheduler_on` choice.')\n",
        "\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "    test_acc = compute_accuracy(model, test_loader, device=device)\n",
        "    print(f'Test accuracy {test_acc :.2f}%')\n",
        "\n",
        "    return minibatch_loss_list, train_acc_list, valid_acc_list"
      ],
      "metadata": {
        "id": "ytE9W_l8FKDe"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = ResNet50().to(device) # ResNet34\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       factor=0.1,\n",
        "                                                       mode='max',\n",
        "                                                       verbose=True)\n",
        "\n",
        "\n",
        "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
        "    model=model,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    train_loader=train_loader,\n",
        "    valid_loader=valid_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    scheduler=scheduler,\n",
        "    scheduler_on='valid_acc',\n",
        "    logging_interval=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjt0V-gP5yUm",
        "outputId": "ea5a8a5d-bd51-400e-f0bc-2f3f4d4cba82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/050 | Batch 0000/0175 | Loss: 2.4990\n",
            "Epoch: 001/050 | Batch 0100/0175 | Loss: 2.2197\n",
            "Epoch: 001/050 | Train: 18.82% | Validation: 18.78%\n",
            "Time elapsed: 3.13 min\n",
            "Epoch: 002/050 | Batch 0000/0175 | Loss: 2.2044\n",
            "Epoch: 002/050 | Batch 0100/0175 | Loss: 2.0196\n",
            "Epoch: 002/050 | Train: 20.33% | Validation: 19.94%\n",
            "Time elapsed: 6.30 min\n",
            "Epoch: 003/050 | Batch 0000/0175 | Loss: 1.9360\n",
            "Epoch: 003/050 | Batch 0100/0175 | Loss: 1.8709\n",
            "Epoch: 003/050 | Train: 34.84% | Validation: 35.04%\n",
            "Time elapsed: 9.48 min\n",
            "Epoch: 004/050 | Batch 0000/0175 | Loss: 1.6679\n",
            "Epoch: 004/050 | Batch 0100/0175 | Loss: 1.5930\n",
            "Epoch: 004/050 | Train: 38.88% | Validation: 38.74%\n",
            "Time elapsed: 12.65 min\n",
            "Epoch: 005/050 | Batch 0000/0175 | Loss: 1.5877\n",
            "Epoch: 005/050 | Batch 0100/0175 | Loss: 1.6040\n",
            "Epoch: 005/050 | Train: 43.85% | Validation: 44.02%\n",
            "Time elapsed: 15.84 min\n",
            "Epoch: 006/050 | Batch 0000/0175 | Loss: 1.4507\n",
            "Epoch: 006/050 | Batch 0100/0175 | Loss: 1.4373\n",
            "Epoch: 006/050 | Train: 48.54% | Validation: 49.20%\n",
            "Time elapsed: 19.02 min\n",
            "Epoch: 007/050 | Batch 0000/0175 | Loss: 1.3373\n",
            "Epoch: 007/050 | Batch 0100/0175 | Loss: 1.4610\n",
            "Epoch: 007/050 | Train: 50.64% | Validation: 51.34%\n",
            "Time elapsed: 22.20 min\n",
            "Epoch: 008/050 | Batch 0000/0175 | Loss: 1.3164\n",
            "Epoch: 008/050 | Batch 0100/0175 | Loss: 1.2259\n",
            "Epoch: 008/050 | Train: 52.57% | Validation: 51.68%\n",
            "Time elapsed: 25.38 min\n",
            "Epoch: 009/050 | Batch 0000/0175 | Loss: 1.3048\n",
            "Epoch: 009/050 | Batch 0100/0175 | Loss: 1.2892\n",
            "Epoch: 009/050 | Train: 56.06% | Validation: 55.20%\n",
            "Time elapsed: 28.54 min\n",
            "Epoch: 010/050 | Batch 0000/0175 | Loss: 1.2074\n",
            "Epoch: 010/050 | Batch 0100/0175 | Loss: 1.3185\n",
            "Epoch: 010/050 | Train: 57.02% | Validation: 56.68%\n",
            "Time elapsed: 31.70 min\n",
            "Epoch: 011/050 | Batch 0000/0175 | Loss: 1.1334\n",
            "Epoch: 011/050 | Batch 0100/0175 | Loss: 1.2082\n",
            "Epoch: 011/050 | Train: 59.56% | Validation: 59.54%\n",
            "Time elapsed: 34.85 min\n",
            "Epoch: 012/050 | Batch 0000/0175 | Loss: 1.1651\n",
            "Epoch: 012/050 | Batch 0100/0175 | Loss: 1.0289\n",
            "Epoch: 012/050 | Train: 63.36% | Validation: 62.90%\n",
            "Time elapsed: 38.01 min\n",
            "Epoch: 013/050 | Batch 0000/0175 | Loss: 1.3151\n",
            "Epoch: 013/050 | Batch 0100/0175 | Loss: 0.9518\n",
            "Epoch: 013/050 | Train: 66.34% | Validation: 64.40%\n",
            "Time elapsed: 41.18 min\n",
            "Epoch: 014/050 | Batch 0000/0175 | Loss: 0.9867\n",
            "Epoch: 014/050 | Batch 0100/0175 | Loss: 0.9464\n",
            "Epoch: 014/050 | Train: 68.24% | Validation: 66.60%\n",
            "Time elapsed: 44.37 min\n",
            "Epoch: 015/050 | Batch 0000/0175 | Loss: 0.8787\n",
            "Epoch: 015/050 | Batch 0100/0175 | Loss: 0.8555\n",
            "Epoch: 015/050 | Train: 70.29% | Validation: 68.28%\n",
            "Time elapsed: 47.54 min\n",
            "Epoch: 016/050 | Batch 0000/0175 | Loss: 0.8406\n",
            "Epoch: 016/050 | Batch 0100/0175 | Loss: 0.8162\n",
            "Epoch: 016/050 | Train: 71.83% | Validation: 69.68%\n",
            "Time elapsed: 50.73 min\n",
            "Epoch: 017/050 | Batch 0000/0175 | Loss: 0.7970\n",
            "Epoch: 017/050 | Batch 0100/0175 | Loss: 0.8393\n",
            "Epoch: 017/050 | Train: 73.50% | Validation: 71.12%\n",
            "Time elapsed: 53.92 min\n",
            "Epoch: 018/050 | Batch 0000/0175 | Loss: 0.6382\n",
            "Epoch: 018/050 | Batch 0100/0175 | Loss: 0.6849\n",
            "Epoch: 018/050 | Train: 75.06% | Validation: 72.06%\n",
            "Time elapsed: 57.15 min\n",
            "Epoch: 019/050 | Batch 0000/0175 | Loss: 0.7001\n",
            "Epoch: 019/050 | Batch 0100/0175 | Loss: 0.6087\n",
            "Epoch: 019/050 | Train: 74.65% | Validation: 72.04%\n",
            "Time elapsed: 60.34 min\n",
            "Epoch: 020/050 | Batch 0000/0175 | Loss: 0.7206\n",
            "Epoch: 020/050 | Batch 0100/0175 | Loss: 0.6529\n",
            "Epoch: 020/050 | Train: 77.73% | Validation: 74.00%\n",
            "Time elapsed: 63.55 min\n",
            "Epoch: 021/050 | Batch 0000/0175 | Loss: 0.5443\n",
            "Epoch: 021/050 | Batch 0100/0175 | Loss: 0.6051\n",
            "Epoch: 021/050 | Train: 78.95% | Validation: 74.92%\n",
            "Time elapsed: 66.76 min\n",
            "Epoch: 022/050 | Batch 0000/0175 | Loss: 0.6949\n",
            "Epoch: 022/050 | Batch 0100/0175 | Loss: 0.6049\n",
            "Epoch: 022/050 | Train: 81.97% | Validation: 76.56%\n",
            "Time elapsed: 69.97 min\n",
            "Epoch: 023/050 | Batch 0000/0175 | Loss: 0.5291\n",
            "Epoch: 023/050 | Batch 0100/0175 | Loss: 0.5055\n",
            "Epoch: 023/050 | Train: 82.07% | Validation: 76.50%\n",
            "Time elapsed: 73.17 min\n",
            "Epoch: 024/050 | Batch 0000/0175 | Loss: 0.4924\n",
            "Epoch: 024/050 | Batch 0100/0175 | Loss: 0.5420\n",
            "Epoch: 024/050 | Train: 81.38% | Validation: 75.46%\n",
            "Time elapsed: 76.37 min\n",
            "Epoch: 025/050 | Batch 0000/0175 | Loss: 0.4452\n",
            "Epoch: 025/050 | Batch 0100/0175 | Loss: 0.4999\n",
            "Epoch: 025/050 | Train: 85.67% | Validation: 78.48%\n",
            "Time elapsed: 79.56 min\n",
            "Epoch: 026/050 | Batch 0000/0175 | Loss: 0.4221\n",
            "Epoch: 026/050 | Batch 0100/0175 | Loss: 0.4042\n",
            "Epoch: 026/050 | Train: 85.58% | Validation: 78.86%\n",
            "Time elapsed: 82.71 min\n",
            "Epoch: 027/050 | Batch 0000/0175 | Loss: 0.3761\n",
            "Epoch: 027/050 | Batch 0100/0175 | Loss: 0.3972\n",
            "Epoch: 027/050 | Train: 87.42% | Validation: 78.64%\n",
            "Time elapsed: 85.86 min\n",
            "Epoch: 028/050 | Batch 0000/0175 | Loss: 0.3339\n",
            "Epoch: 028/050 | Batch 0100/0175 | Loss: 0.4007\n",
            "Epoch: 028/050 | Train: 87.75% | Validation: 79.04%\n",
            "Time elapsed: 89.03 min\n",
            "Epoch: 029/050 | Batch 0000/0175 | Loss: 0.3084\n",
            "Epoch: 029/050 | Batch 0100/0175 | Loss: 0.3419\n",
            "Epoch: 029/050 | Train: 87.59% | Validation: 79.00%\n",
            "Time elapsed: 92.18 min\n",
            "Epoch: 030/050 | Batch 0000/0175 | Loss: 0.2827\n",
            "Epoch: 030/050 | Batch 0100/0175 | Loss: 0.3232\n",
            "Epoch: 030/050 | Train: 87.73% | Validation: 79.06%\n",
            "Time elapsed: 95.34 min\n",
            "Epoch: 031/050 | Batch 0000/0175 | Loss: 0.2934\n",
            "Epoch: 031/050 | Batch 0100/0175 | Loss: 0.4220\n",
            "Epoch: 031/050 | Train: 90.39% | Validation: 79.44%\n",
            "Time elapsed: 98.50 min\n",
            "Epoch: 032/050 | Batch 0000/0175 | Loss: 0.2400\n",
            "Epoch: 032/050 | Batch 0100/0175 | Loss: 0.2583\n",
            "Epoch: 032/050 | Train: 86.59% | Validation: 77.38%\n",
            "Time elapsed: 101.65 min\n",
            "Epoch: 033/050 | Batch 0000/0175 | Loss: 0.2829\n",
            "Epoch: 033/050 | Batch 0100/0175 | Loss: 0.3013\n",
            "Epoch: 033/050 | Train: 91.56% | Validation: 80.38%\n",
            "Time elapsed: 104.82 min\n",
            "Epoch: 034/050 | Batch 0000/0175 | Loss: 0.2462\n",
            "Epoch: 034/050 | Batch 0100/0175 | Loss: 0.1904\n",
            "Epoch: 034/050 | Train: 91.69% | Validation: 80.36%\n",
            "Time elapsed: 108.08 min\n",
            "Epoch: 035/050 | Batch 0000/0175 | Loss: 0.2214\n",
            "Epoch: 035/050 | Batch 0100/0175 | Loss: 0.2384\n",
            "Epoch: 035/050 | Train: 91.59% | Validation: 80.10%\n",
            "Time elapsed: 111.25 min\n",
            "Epoch: 036/050 | Batch 0000/0175 | Loss: 0.2164\n",
            "Epoch: 036/050 | Batch 0100/0175 | Loss: 0.2807\n",
            "Epoch: 036/050 | Train: 93.37% | Validation: 80.88%\n",
            "Time elapsed: 114.42 min\n",
            "Epoch: 037/050 | Batch 0000/0175 | Loss: 0.1858\n",
            "Epoch: 037/050 | Batch 0100/0175 | Loss: 0.1680\n",
            "Epoch: 037/050 | Train: 93.48% | Validation: 80.98%\n",
            "Time elapsed: 117.57 min\n",
            "Epoch: 038/050 | Batch 0000/0175 | Loss: 0.1554\n",
            "Epoch: 038/050 | Batch 0100/0175 | Loss: 0.2821\n",
            "Epoch: 038/050 | Train: 91.37% | Validation: 79.72%\n",
            "Time elapsed: 120.73 min\n",
            "Epoch: 039/050 | Batch 0000/0175 | Loss: 0.1441\n",
            "Epoch: 039/050 | Batch 0100/0175 | Loss: 0.1525\n",
            "Epoch: 039/050 | Train: 93.40% | Validation: 80.76%\n",
            "Time elapsed: 123.87 min\n",
            "Epoch: 040/050 | Batch 0000/0175 | Loss: 0.1325\n",
            "Epoch: 040/050 | Batch 0100/0175 | Loss: 0.2054\n",
            "Epoch: 040/050 | Train: 93.68% | Validation: 80.38%\n",
            "Time elapsed: 127.00 min\n",
            "Epoch: 041/050 | Batch 0000/0175 | Loss: 0.0999\n",
            "Epoch: 041/050 | Batch 0100/0175 | Loss: 0.1962\n",
            "Epoch: 041/050 | Train: 94.03% | Validation: 80.86%\n",
            "Time elapsed: 130.14 min\n",
            "Epoch: 042/050 | Batch 0000/0175 | Loss: 0.0852\n",
            "Epoch: 042/050 | Batch 0100/0175 | Loss: 0.1422\n",
            "Epoch: 042/050 | Train: 95.22% | Validation: 81.72%\n",
            "Time elapsed: 133.29 min\n",
            "Epoch: 043/050 | Batch 0000/0175 | Loss: 0.0895\n",
            "Epoch: 043/050 | Batch 0100/0175 | Loss: 0.1104\n",
            "Epoch: 043/050 | Train: 95.14% | Validation: 81.78%\n",
            "Time elapsed: 136.41 min\n",
            "Epoch: 044/050 | Batch 0000/0175 | Loss: 0.1328\n",
            "Epoch: 044/050 | Batch 0100/0175 | Loss: 0.0830\n",
            "Epoch: 044/050 | Train: 96.18% | Validation: 81.76%\n",
            "Time elapsed: 139.53 min\n",
            "Epoch: 045/050 | Batch 0000/0175 | Loss: 0.1121\n",
            "Epoch: 045/050 | Batch 0100/0175 | Loss: 0.1648\n",
            "Epoch: 045/050 | Train: 94.71% | Validation: 80.40%\n",
            "Time elapsed: 142.67 min\n",
            "Epoch: 046/050 | Batch 0000/0175 | Loss: 0.1011\n",
            "Epoch: 046/050 | Batch 0100/0175 | Loss: 0.2031\n",
            "Epoch: 046/050 | Train: 95.61% | Validation: 80.96%\n",
            "Time elapsed: 145.83 min\n",
            "Epoch: 047/050 | Batch 0000/0175 | Loss: 0.1064\n",
            "Epoch: 047/050 | Batch 0100/0175 | Loss: 0.0878\n",
            "Epoch: 047/050 | Train: 96.35% | Validation: 81.78%\n",
            "Time elapsed: 148.96 min\n",
            "Epoch: 048/050 | Batch 0000/0175 | Loss: 0.1019\n",
            "Epoch: 048/050 | Batch 0100/0175 | Loss: 0.0902\n",
            "Epoch: 048/050 | Train: 96.29% | Validation: 81.42%\n",
            "Time elapsed: 152.10 min\n",
            "Epoch: 049/050 | Batch 0000/0175 | Loss: 0.1152\n",
            "Epoch: 049/050 | Batch 0100/0175 | Loss: 0.0901\n",
            "Epoch: 049/050 | Train: 97.13% | Validation: 82.44%\n",
            "Time elapsed: 155.33 min\n",
            "Epoch: 050/050 | Batch 0000/0175 | Loss: 0.1645\n",
            "Epoch: 050/050 | Batch 0100/0175 | Loss: 0.0837\n"
          ]
        }
      ]
    }
  ]
}